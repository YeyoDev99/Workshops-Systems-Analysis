# Workshop 2 ‚Äî System Design (Summary)

## Objective
Build upon the systems analysis from **Workshop 1** to create a robust **system design** that addresses challenges, requirements, and chaotic/sensitive aspects identified earlier.

---

## Key Tasks

### 1. Review of Workshop 1 Findings
- Summarize critical constraints, dataset characteristics, and chaos/sensitivity aspects.
- Ensure the proposed design directly addresses these findings.

### 2. Define System Requirements
- Translate analysis into measurable design requirements (performance, reliability, scalability).
- Consider user needs: interpretability, usability, and security.

### 3. High-Level Architecture
- Propose an architectural blueprint including:
  - **Data ingestion**
  - **Preprocessing/ETL**
  - **Feature engineering**
  - **Model training & validation**
  - **Deployment & monitoring**
- Apply **systems engineering principles** (modularity, scalability, maintainability).

### 4. Sensitivity & Chaos Mitigation
- Strategies to handle sensitive inputs, noisy data, and feedback loops.
- Monitoring, error handling, and stability mechanisms for unpredictable conditions.

### 5. Technical Stack & Implementation
- Recommend tools/frameworks (e.g., Python, scikit-learn, Pytorch).
- Justify choices in terms of scalability, reproducibility, and maintainability.
- Outline integration plan and potential design patterns.

---

## **Workshop 2 Development Process Update** üõ†Ô∏è

The system design was developed following the principles of **Modular Monolith** architecture and incorporating strategies to mitigate the non-linear "chaos factors" identified in Workshop 1 (e.g., viral spikes, ambiguous missing data).

### Key Design Outcomes:

* **Architecture:** A **Chain of Responsibility** pattern was implemented for the data flow, ensuring strict sequential execution and data validation between the 8 main modules (Ingestion to Evaluation).
* **Chaos Mitigation:** A **Hierarchical Ensemble** approach was designed, using the **Strategy Pattern** to dynamically select the best base model (ARIMA, Prophet, LSTM) based on the specific characteristics and volatility of each Wikipedia article.
* **Scalability:** The stack relies on **Python** with parallel processing tools like **Joblib** to meet the NFR of processing 145,000 time series efficiently.
* **Design Diagrams:** Comprehensive **Architecture Diagrams** (High-Level Flow, Subsystem Breakdown for Training, and Subsystem Breakdown for Inference/Evaluation) have been created and are referenced in the final PDF to illustrate the system's modularity and iterative feedback loops.

---

## Deliverables

- **System Design Document (PDF)** ‚Üí stored in \texttt{workshop2/} (or \texttt{Workshop\_2\_Design/}).
- **README.md update** ‚Üí explaining process, diagrams, and linking to final PDF.
- **Diagrams** ‚Üí System architecture, data flow, and module interactions.

\textbf{[Link to Final PDF Document: Workshop\_2\_System\_Design.pdf]}

---

## Deadline
üìÖ **October 18th, 2025 ‚Äî 12:00 PM**
Late submissions may incur penalties.

---

## Notes
- Report must be in **English** and in **PDF format**.
- Must reference **Workshop 1 outcomes**.
- Emphasize how **chaos theory** and **sensitivity analysis** informed the design.
- The use of the \texttt{ydata-profiling} tool (and associated \texttt{requirements.txt}) demonstrates the initial commitment to **Data Quality** and **EDA** in the Python stack.